{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T03:52:02.306938Z",
     "start_time": "2019-12-06T03:52:02.123830Z"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://static.leiphone.com/%E7%BE%8E%E9%A3%9F%E6%8C%91%E6%88%98%EF%BC%881%EF%BC%89%E8%B1%86%E8%85%90%E5%92%8C%E5%9C%9F%E8%B1%86.zip    \n",
    "#下载数据集\n",
    "#Download dataset\n",
    "!unzip 美食挑战（1）豆腐和土豆.zip\n",
    "!rm 美食挑战（1）豆腐和土豆.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-06 20:58:48--  https://www.cs.toronto.edu/~frossard/vgg16/vgg16_weights.npz\n",
      "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
      "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 553436134 (528M)\n",
      "Saving to: ‘vgg16_weights.npz’\n",
      "\n",
      "100%[======================================>] 553,436,134 2.39MB/s   in 3m 47s \n",
      "\n",
      "2019-12-06 21:02:40 (2.33 MB/s) - ‘vgg16_weights.npz’ saved [553436134/553436134]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://www.cs.toronto.edu/~frossard/vgg16/vgg16_weights.npz\n",
    "#Download model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T15:15:09.380633Z",
     "start_time": "2019-12-06T15:15:09.375560Z"
    }
   },
   "source": [
    "2.构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T12:43:45.351282Z",
     "start_time": "2019-12-06T12:43:45.343974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from time import time\n",
    "import VGG16_model as model\n",
    "import utils\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]='3'\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T12:46:22.105908Z",
     "start_time": "2019-12-06T12:46:20.724280Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7502\n",
      "7502\n",
      "./train/0.jpg ./train/7501.jpg\n",
      "WARNING:tensorflow:From /SkyDiscovery/cephfs/user/fehng/toufu_potato/utils.py:36: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/SkyCompute/lib/python3.6/site-packages/tensorflow/python/training/input.py:372: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/SkyCompute/lib/python3.6/site-packages/tensorflow/python/training/input.py:318: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/SkyCompute/lib/python3.6/site-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /usr/local/SkyCompute/lib/python3.6/site-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/SkyCompute/lib/python3.6/site-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /SkyDiscovery/cephfs/user/fehng/toufu_potato/utils.py:41: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From <ipython-input-2-66924eeb1d54>:15: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "--------weights loaded-----------\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "image_dir=\"./train\"\n",
    "label_path=\"train.csv\"\n",
    "batch_size=50\n",
    "capacity=50\n",
    "sess=tf.Session()\n",
    "\n",
    "xs,ys=utils.get_file(image_dir,label_path)\n",
    "print(xs[0],xs[-1])\n",
    "image_batch,label_batch=utils.get_batch(xs,ys,224,224,batch_size,capacity)\n",
    "x=tf.placeholder(tf.float32,[None,224,224,3])\n",
    "y=tf.placeholder(tf.int32,[None,2])\n",
    "vgg=model.vgg16(x)\n",
    "fc8_dinetuining=vgg.probs\n",
    "loss_function=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=fc8_dinetuining,labels=y))\n",
    "prediction=tf.argmax(fc8_dinetuining,1)\n",
    "correct_prediction=tf.equal(tf.argmax(y,1),tf.argmax(fc8_dinetuining,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss_function)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "vgg.load_weights('vgg16_weights.npz',sess)\n",
    "saver=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-06T12:46:39.068Z"
    }
   },
   "outputs": [],
   "source": [
    "ckpt_dir='./model/'\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "saver=tf.train.Saver(max_to_keep=3)\n",
    "ckpt=tf.train.latest_checkpoint(ckpt_dir)\n",
    "if ckpt!=None:\n",
    "    saver.restore(sess,ckpt)\n",
    "else:\n",
    "    print(\"Training from scratch!!\")\n",
    "\n",
    "coord=tf.train.Coordinator()\n",
    "threads=tf.train.start_queue_runners(coord=coord,sess=sess)\n",
    "epoch_start_time=time()\n",
    "acc_list=[]\n",
    "loss_list=[]\n",
    "for i in range(1000):\n",
    "    images,labels=sess.run([image_batch,label_batch])\n",
    "    labels=utils.onehot(labels)\n",
    "    sess.run(optimizer,feed_dict={x:images,y:labels})\n",
    "    loss,acc=sess.run([loss_function,accuracy],feed_dict={x:images,y:labels})\n",
    "    acc_list.append(acc)\n",
    "    loss_list.append(loss)\n",
    "    #print(\"Now the loss is %f\"%loss)\n",
    "    #print(\"Now the acc is %f\"%np.mean(acc_list))\n",
    "    epoch_end_time=time()\n",
    "    #print(\"epoch takes:\",(epoch_end_time-epoch_start_time))\n",
    "    epoch_start_time=epoch_end_time\n",
    "    if (i+1)%50==0:\n",
    "        print(\"Now the loss is %f\"%np.mean(loss_list))\n",
    "        print(\"Now the acc is %f\"%np.mean(acc_list))\n",
    "        acc_list=[]\n",
    "        loss_list=[]\n",
    "        print(\"----------Epoch %d id Finished --------\"%i)\n",
    "    if (i+1)%500==0:\n",
    "        saver.save(sess,\"./model/tp.ckpt\")\n",
    "    \n",
    "saver.save(sess,'./model/')\n",
    "print(\"Optimizer Finished\")\n",
    "duration=time()-startTime\n",
    "print(\"Total takes:{:.2f}\".format(duration))\n",
    "coord.request_stop()#通知其他线程关闭\n",
    "coord.join(threads)#等待所有线程关闭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/tp.ckpt\n",
      "Model restoring\n",
      "(1047,)\n",
      "[0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#预测\n",
    "import PIL.Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "tf.reset_default_graph()\n",
    "means=[123.68,116.779,103.939]\n",
    "x=tf.placeholder(tf.float32,[None,224,224,3])\n",
    "sess=tf.Session()\n",
    "vgg=model.vgg16(x)\n",
    "fc8_dinetuining=vgg.probs\n",
    "saver=tf.train.Saver()\n",
    "ckpt_dir='./model/'\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "ckpt=tf.train.latest_checkpoint(ckpt_dir)\n",
    "if ckpt!=None:\n",
    "    saver.restore(sess,ckpt)\n",
    "else:\n",
    "    print(\"Training from scratch!!\")\n",
    "print(\"Model restoring\")\n",
    "#saver.restore(sess,'./model/')#默认恢复最后保存的模型\n",
    "#saver.restore(sess,'./model/epoch_0008000.ckpt)\n",
    "#恢复指定检查点的模型\n",
    "label_list=[]\n",
    "for i in range(1047):\n",
    "    img_path='./test/{:}.jpg'.format(i)\n",
    "    img=PIL.Image.open(img_path)\n",
    "    img=np.float32(img.resize((224,224),PIL.Image.ANTIALIAS))\n",
    "    for c in range(3):\n",
    "        img[:,:,c]-=means[c]\n",
    "    #print(img)\n",
    "    imgs=np.array([img],dtype=np.float32)\n",
    "    #print(imgs.shape)\n",
    "    prob=sess.run(fc8_dinetuining,feed_dict={x:imgs})\n",
    "    max_index=np.argmax(prob)\n",
    "    label_list.append(max_index)\n",
    "result=np.array(label_list,dtype=np.uint8)\n",
    "print(result.shape)\n",
    "print(label_list)\n",
    "data1 = pd.DataFrame(result)\n",
    "data1.to_csv('test_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch test_label.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
